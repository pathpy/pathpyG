{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import pathpyG as pp\n",
    "\n",
    "print('Running on', pp.config['torch']['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put this as his in conftest as 'simple_paths_centralities'\n",
    "paths = pp.PathData()\n",
    "paths.add_walk(torch.tensor([[2, 1, 3], [1, 3, 5]]))  \n",
    "paths.add_walk(torch.tensor([[0, 1], [1, 3]]))  \n",
    "paths.add_walk(torch.tensor([[3], [4]]))  \n",
    "\n",
    "\n",
    "simple_paths_centralities = paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = pp.PathData()\n",
    "# paths.add_walk(torch.tensor([[0,2,3],[2,3,4]]),freq=3) # A -> C -> D\n",
    "# paths.add_walk(torch.tensor([[0,2],[2,3]])) # A -> C -> D\n",
    "# paths.add_walk(torch.tensor([[1,2],[2,4]])) # B -> C -> E\n",
    "# paths.add_walk(torch.tensor([[4],[5]]))\n",
    "# paths.add_walk(torch.tensor([[1,2],[2,4]])) # B -> C -> E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0, 1],\n",
       "          [1, 3],\n",
       "          [2, 1]],\n",
       " \n",
       "         [[1, 3],\n",
       "          [3, 5],\n",
       "          [1, 3]]]),\n",
       " tensor([1., 1., 1.]))"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index, edge_weights = paths.edge_index_k_weighted(k=2)\n",
    "index, edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, edge_weights = paths.edge_index_k_weighted(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.node_traversals.<locals>.<lambda>()>,\n",
       "            {2: 1, 1: 2, 3: 3, 5: 1, 0: 1, 4: 1})"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "def node_traversals(paths):\n",
    "    \"\"\"Calculates the number of times any path traverses each of the nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paths: Paths\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    \"\"\"\n",
    "    traversals = defaultdict(lambda: 0)\n",
    "    for path_id, path_edgelist in paths.paths.items():\n",
    "        path_seq = paths.walk_to_node_seq(path_edgelist)\n",
    "        for node in path_seq:\n",
    "            traversals[node.item()] += paths.path_freq[path_id]\n",
    "    return traversals\n",
    "node_traversals(paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_node_traversals(simple_paths_centralities):\n",
    "    traversals_dict = node_traversals(simple_paths_centralities)\n",
    "    assert set(traversals_dict.keys()) == {0,1,2,3,4,5}\n",
    "    assert traversals_dict[0] == 1\n",
    "    assert traversals_dict[1] == 2\n",
    "    assert traversals_dict[2] == 1\n",
    "    assert traversals_dict[3] == 3\n",
    "    assert traversals_dict[4] == 1\n",
    "    assert traversals_dict[5] == 1\n",
    "\n",
    "test_node_traversals(simple_paths_centralities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.node_traversals.<locals>.<lambda>()>,\n",
       "            {2: 0.1111111111111111,\n",
       "             1: 0.2222222222222222,\n",
       "             3: 0.3333333333333333,\n",
       "             5: 0.1111111111111111,\n",
       "             0: 0.1111111111111111,\n",
       "             4: 0.1111111111111111})"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def visitation_probabilities(paths):\n",
    "    \"\"\"Calculates the probabilities that a randomly chosen path passes through each of\n",
    "    the nodes. If 5 out of 100 paths (of any length) traverse node v, node v will be\n",
    "    assigned a visitation probability of 0.05. This measure can be interpreted as ground\n",
    "    truth for the notion of importance captured by PageRank applied to a graphical\n",
    "    abstraction of the paths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paths: Paths\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    \"\"\"\n",
    "    if not isinstance(paths, pp.PathData):\n",
    "        assert False, \"`paths` must be an instance of Paths\"\n",
    "    # Log.add('Calculating visitation probabilities...', Severity.INFO)\n",
    "\n",
    "    # entries capture the probability that a given node is visited on an arbitrary path\n",
    "    # Note: this is identical to the subpath count of zero-length paths\n",
    "    # (i.e. the relative frequencies of nodes across all pathways)\n",
    "    visit_probabilities = node_traversals(paths)\n",
    "\n",
    "    # total number of visits\n",
    "    visits = 0.0\n",
    "    for v in visit_probabilities:\n",
    "        visits += visit_probabilities[v]\n",
    "\n",
    "    for v in visit_probabilities:\n",
    "        visit_probabilities[v] /= visits\n",
    "    # Log.add('finished.', Severity.INFO)\n",
    "    return visit_probabilities\n",
    "\n",
    "visitation_probabilities(paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_visitation_probabilities(simple_paths_centralities):\n",
    "    traversals_dict = visitation_probabilities(simple_paths_centralities)\n",
    "    assert set(traversals_dict.keys()) == {0,1,2,3,4,5}\n",
    "    assert traversals_dict[0] == 1/9\n",
    "    assert traversals_dict[1] == 2/9\n",
    "    assert traversals_dict[2] == 1/9\n",
    "    assert traversals_dict[3] == 3/9\n",
    "    assert traversals_dict[4] == 1/9\n",
    "    assert traversals_dict[5] == 1/9\n",
    "\n",
    "test_visitation_probabilities(simple_paths_centralities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError occurred. Reached maximum path length of 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.shortest_paths.<locals>.<lambda>()>,\n",
       "            {0: defaultdict(set,\n",
       "                         {1: {tensor([0, 1])}, 3: {tensor([0, 1, 3])}}),\n",
       "             1: defaultdict(set,\n",
       "                         {3: {tensor([1, 3])}, 5: {tensor([1, 3, 5])}}),\n",
       "             2: defaultdict(set,\n",
       "                         {1: {tensor([2, 1])},\n",
       "                          3: {tensor([2, 1, 3])},\n",
       "                          5: {tensor([2, 1, 3, 5])}}),\n",
       "             3: defaultdict(set, {4: {tensor([3, 4])}, 5: {tensor([3, 5])}})})"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as _np\n",
    "\n",
    "def shortest_paths(paths):\n",
    "    \"\"\"\n",
    "    Calculates all shortest paths between all pairs of nodes \n",
    "    based on a set of empirically observed paths.\n",
    "    \"\"\"\n",
    "    s_p = defaultdict(lambda: defaultdict(set))\n",
    "    s_p_lengths = defaultdict(lambda: defaultdict(lambda: _np.inf))\n",
    "\n",
    "    p_length = 1\n",
    "    index, edge_weights = paths.edge_index_k_weighted(k=p_length)\n",
    "    sources = index[0]\n",
    "    destinations = index[-1]\n",
    "    for e, (s, d) in enumerate(zip(sources, destinations)):\n",
    "        s = s.item()\n",
    "        d = d.item()\n",
    "        s_p_lengths[s][d] = p_length\n",
    "        s_p[s][d] = set({torch.tensor([s,d])})\n",
    "    p_length += 1\n",
    "    while True: # until max path length\n",
    "        try:\n",
    "            index, edge_weights = paths.edge_index_k_weighted(k=p_length)\n",
    "            sources = index[0, :, 0]\n",
    "            destinations = index[1, :, -1]\n",
    "            for e, (s, d) in enumerate(zip(sources, destinations)):\n",
    "                s = s.item()\n",
    "                d = d.item()\n",
    "                if p_length < s_p_lengths[s][d]:\n",
    "                    # update shortest path length\n",
    "                    s_p_lengths[s][d] = p_length\n",
    "                    # redefine set\n",
    "                    s_p[s][d] = {paths.walk_to_node_seq(index[:, e])}\n",
    "                elif p_length == s_p_lengths[s][d]:\n",
    "                    s_p[s][d].add(paths.walk_to_node_seq(index[:, e]))\n",
    "            p_length += 1\n",
    "        except IndexError:\n",
    "            print(f\"IndexError occurred. Reached maximum path length of {p_length}\")\n",
    "            break\n",
    "    return s_p\n",
    "shortest_paths(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError occurred. Reached maximum path length of 4\n"
     ]
    }
   ],
   "source": [
    "def test_shortest_paths(simple_paths_centralities):\n",
    "    s_p = shortest_paths(simple_paths_centralities)\n",
    "    # need to check equality of set of tensors\n",
    "    # checking for paths longer than 1\n",
    "    assert all(torch.equal(tensor1, tensor2) for tensor1, tensor2 in zip(s_p[0][3], {torch.tensor([0, 1, 3])}))\n",
    "    assert all(torch.equal(tensor1, tensor2) for tensor1, tensor2 in zip(s_p[1][5], {torch.tensor([1, 3, 5])}))\n",
    "    assert all(torch.equal(tensor1, tensor2) for tensor1, tensor2 in zip(s_p[2][3], {torch.tensor([2, 1, 3])}))\n",
    "    assert all(torch.equal(tensor1, tensor2) for tensor1, tensor2 in zip(s_p[2][5], {torch.tensor([2, 1, 3, 5])}))\n",
    "\n",
    "test_shortest_paths(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError occurred. Reached maximum path length of 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.betweenness.<locals>.<lambda>()>,\n",
       "            {1: 3.0, 3: 2.0, 0: 0, 2: 0, 4: 0, 5: 0})"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @betweenness.register(Paths)\n",
    "def betweenness(paths, normalized=False):\n",
    "    \"\"\"Calculates the betweenness of nodes based on observed shortest paths\n",
    "    between all pairs of nodes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paths:\n",
    "        Paths object\n",
    "    normalized: bool\n",
    "        normalize such that largest value is 1.0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    \"\"\"\n",
    "    assert isinstance(paths, pp.PathData), \"argument must be an instance of pathpy.Paths\"\n",
    "    node_centralities = defaultdict(lambda: 0)\n",
    "\n",
    "    # Log.add('Calculating betweenness in paths ...', Severity.INFO)\n",
    "\n",
    "    all_paths = shortest_paths(paths)\n",
    "\n",
    "    for s in all_paths:\n",
    "        for d in all_paths[s]:\n",
    "            for p in all_paths[s][d]:\n",
    "                for x in p[1:-1]:\n",
    "                    if s != d != x:\n",
    "                        node_centralities[x.item()] += 1.0 / len(all_paths[s][d])\n",
    "    if normalized:\n",
    "        max_centr = max(node_centralities.values())\n",
    "        for v in node_centralities:\n",
    "            node_centralities[v] /= max_centr\n",
    "    # assign zero values to nodes not occurring on shortest paths\n",
    "    nodes = [v.item() for v in paths.edge_index.reshape(-1).unique(dim=0)]\n",
    "    for v in nodes:\n",
    "        node_centralities[v] += 0\n",
    "    # Log.add('finished.')\n",
    "    return node_centralities\n",
    "\n",
    "betweenness(paths,normalized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError occurred. Reached maximum path length of 4\n"
     ]
    }
   ],
   "source": [
    "def test_betweenness_paths(simple_paths_centralities):\n",
    "    bw = betweenness(simple_paths_centralities,normalized=False)\n",
    "    # 1 is in the shortest path between 0-5,2-3,2-5\n",
    "    assert bw[1] == 3.0\n",
    "    # 1 is in the shortest path between 2-5,1-5\n",
    "    assert bw[3] == 2.0\n",
    "\n",
    "test_betweenness_paths(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError occurred. Reached maximum path length of 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.distance_matrix.<locals>.<lambda>()>,\n",
       "            {0: defaultdict(<function __main__.distance_matrix.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {0: 0, 1: 1, 3: 2}),\n",
       "             1: defaultdict(<function __main__.distance_matrix.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {1: 0, 3: 1, 5: 2}),\n",
       "             2: defaultdict(<function __main__.distance_matrix.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {2: 0, 1: 1, 3: 2, 5: 3}),\n",
       "             3: defaultdict(<function __main__.distance_matrix.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {3: 0, 4: 1, 5: 1}),\n",
       "             4: defaultdict(<function __main__.distance_matrix.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {4: 0}),\n",
       "             5: defaultdict(<function __main__.distance_matrix.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {5: 0})})"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def distance_matrix(paths):\n",
    "    \"\"\"\n",
    "    Calculates shortest path distances between all pairs of\n",
    "    nodes based on the observed shortest paths (and subpaths)\n",
    "    \"\"\"\n",
    "    dist = defaultdict(lambda: defaultdict(lambda: _np.inf))\n",
    "    # Log.add('Calculating distance matrix based on empirical paths ...', Severity.INFO)\n",
    "    nodes = [v.item() for v in paths.edge_index.reshape(-1).unique(dim=0)] # NOTE: modify once set of nodes can be obtained from path obeject\n",
    "    for v in nodes:\n",
    "        dist[v][v] = 0\n",
    "\n",
    "    p_length = 1\n",
    "    index, edge_weights = paths.edge_index_k_weighted(k=p_length)\n",
    "    sources = index[0]\n",
    "    destinations = index[-1]\n",
    "    for e, (s, d) in enumerate(zip(sources, destinations)):\n",
    "        s = s.item()\n",
    "        d = d.item()\n",
    "        dist[s][d] = p_length\n",
    "        # s_p[s][d] = set({torch.tensor([s,d])})\n",
    "    p_length += 1\n",
    "    while True: # until max path length\n",
    "        try:\n",
    "            index, edge_weights = paths.edge_index_k_weighted(k=p_length)\n",
    "            sources = index[0, :, 0]\n",
    "            destinations = index[1, :, -1]\n",
    "            for e, (s, d) in enumerate(zip(sources, destinations)):\n",
    "                s = s.item()\n",
    "                d = d.item()\n",
    "                if p_length < dist[s][d]:\n",
    "                    # update shortest path length\n",
    "                    dist[s][d] = p_length\n",
    "            p_length += 1\n",
    "        except IndexError:\n",
    "            print(f\"IndexError occurred. Reached maximum path length of {p_length}\")\n",
    "            break\n",
    "    return dist\n",
    "distance_matrix(paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError occurred. Reached maximum path length of 4\n"
     ]
    }
   ],
   "source": [
    "def test_distance_matrix_paths(simple_paths_centralities):\n",
    "    dm = distance_matrix(simple_paths_centralities)\n",
    "    assert dm[0] == {0: 0, 1: 1, 3: 2}\n",
    "    assert dm[1] == {1: 0, 3: 1, 5: 2}\n",
    "    assert dm[2] == {2: 0, 1: 1, 3: 2, 5: 3}\n",
    "    assert dm[3] == {3: 0, 4: 1, 5: 1}\n",
    "    assert dm[4] == {4: 0}\n",
    "    assert dm[5] == {5: 0}\n",
    "\n",
    "test_distance_matrix_paths(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError occurred. Reached maximum path length of 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.closeness.<locals>.<lambda>()>,\n",
       "            {1: 2.0, 3: 2.0, 4: 1.0, 5: 1.8333333333333333, 0: 0.0, 2: 0.0})"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def closeness(paths, normalized=False):\n",
    "    \"\"\"Calculates the closeness of nodes based on observed shortest paths\n",
    "    between all nodes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paths: Paths\n",
    "    normalized: bool\n",
    "        normalize such that largest value is 1.0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    \"\"\"\n",
    "    node_centralities = defaultdict(lambda: 0)\n",
    "    distances = distance_matrix(paths)\n",
    "    nodes = [v.item() for v in paths.edge_index.reshape(-1).unique(dim=0)] # NOTE: modify once set of nodes can be obtained from path obeject\n",
    "\n",
    "    for x in nodes:\n",
    "        # calculate closeness centrality of x\n",
    "        for d in nodes:\n",
    "            if x != d and distances[d][x] < _np.inf:\n",
    "                node_centralities[x] += 1.0 / distances[d][x]\n",
    "\n",
    "    # assign zero values to nodes not occurring\n",
    "    \n",
    "    for v in nodes:\n",
    "        node_centralities[v] += 0.0\n",
    "\n",
    "    if normalized:\n",
    "        m = max(node_centralities.values())\n",
    "        for v in nodes:\n",
    "            node_centralities[v] /= m\n",
    "\n",
    "    return node_centralities\n",
    "closeness(paths, normalized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError occurred. Reached maximum path length of 4\n"
     ]
    }
   ],
   "source": [
    "def test_closeness_paths(simple_paths_centralities):\n",
    "    c = closeness(simple_paths_centralities, normalized=False)\n",
    "    assert c[0] == 0.0\n",
    "    # 1 reachable from 0 and 2 in one step\n",
    "    assert c[1] == 1/1 + 1/1\n",
    "    assert c[2] == 0\n",
    "    # 3 reachable from 1 in one step, from 0 and 3 in two steps\n",
    "    assert c[3] == 1 + 1/2 + 1/2\n",
    "    assert c[4] == 1\n",
    "    # 5 reachable from 3 in one step, from 1 in two steps, from 2 in three steps\n",
    "    assert c[5] == 1 + 1/2 + 1/3\n",
    "test_closeness_paths(paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
