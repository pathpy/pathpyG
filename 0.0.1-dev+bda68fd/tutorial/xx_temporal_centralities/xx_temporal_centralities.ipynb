{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import pathpyG as pp\n",
    "\n",
    "print('Running on', pp.config['torch']['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = pp.PathData()\n",
    "paths.add_walk(torch.tensor([[0,2,3],[2,3,4]]),freq=3) # A -> C -> D\n",
    "paths.add_walk(torch.tensor([[0,2],[2,3]])) # A -> C -> D\n",
    "paths.add_walk(torch.tensor([[1,2],[2,4]])) # B -> C -> E\n",
    "paths.add_walk(torch.tensor([[4],[5]]))\n",
    "# paths.add_walk(torch.tensor([[1,2],[2,4]])) # B -> C -> E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0, 2],\n",
       "          [1, 2],\n",
       "          [2, 3]],\n",
       " \n",
       "         [[2, 3],\n",
       "          [2, 4],\n",
       "          [3, 4]]]),\n",
       " tensor([4., 1., 3.]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index, edge_weights = paths.edge_index_k_weighted(k=2)\n",
    "index, edge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 2, 3],\n",
       "         [2, 3, 4]],\n",
       "\n",
       "        [[2, 3, 4],\n",
       "         [3, 4, 5]],\n",
       "\n",
       "        [[2, 3, 4],\n",
       "         [3, 4, 5]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=3\n",
    "torch.tensor([[0,2,3,4],[2,3,4,5]]).unfold(1,k,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, edge_weights = paths.edge_index_k_weighted(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.node_traversals.<locals>.<lambda>()>,\n",
       "            {0: 0.3333333333333333,\n",
       "             2: 0.3333333333333333,\n",
       "             3: 0.3333333333333333})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "def node_traversals(paths):\n",
    "    \"\"\"Calculates the number of times any path traverses each of the nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paths: Paths\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    \"\"\"\n",
    "    traversals = defaultdict(lambda: 0)\n",
    "    for path_id, path_edgelist in paths.paths.items():\n",
    "        path_seq = paths.walk_to_node_seq(path_edgelist)\n",
    "        for node in path_seq:\n",
    "            traversals[node.item()] += paths.path_freq[path_id]\n",
    "    return traversals\n",
    "# node_traversals(paths)\n",
    "\n",
    "def visitation_probabilities(paths):\n",
    "    \"\"\"Calculates the probabilities that a randomly chosen path passes through each of\n",
    "    the nodes. If 5 out of 100 paths (of any length) traverse node v, node v will be\n",
    "    assigned a visitation probability of 0.05. This measure can be interpreted as ground\n",
    "    truth for the notion of importance captured by PageRank applied to a graphical\n",
    "    abstraction of the paths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paths: Paths\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    \"\"\"\n",
    "    if not isinstance(paths, pp.PathData):\n",
    "        assert False, \"`paths` must be an instance of Paths\"\n",
    "    # Log.add('Calculating visitation probabilities...', Severity.INFO)\n",
    "\n",
    "    # entries capture the probability that a given node is visited on an arbitrary path\n",
    "    # Note: this is identical to the subpath count of zero-length paths\n",
    "    # (i.e. the relative frequencies of nodes across all pathways)\n",
    "    visit_probabilities = node_traversals(paths)\n",
    "\n",
    "    # total number of visits\n",
    "    visits = 0.0\n",
    "    for v in visit_probabilities:\n",
    "        visits += visit_probabilities[v]\n",
    "\n",
    "    for v in visit_probabilities:\n",
    "        visit_probabilities[v] /= visits\n",
    "    # Log.add('finished.', Severity.INFO)\n",
    "    return visit_probabilities\n",
    "\n",
    "visitation_probabilities(paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError occurred. Reached maximum path length of 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.shortest_paths.<locals>.<lambda>()>,\n",
       "            {0: defaultdict(set,\n",
       "                         {2: {tensor([0, 2])},\n",
       "                          3: {tensor([0, 2, 3])},\n",
       "                          4: {tensor([0, 2, 3, 4])}}),\n",
       "             1: defaultdict(set,\n",
       "                         {2: {tensor([1, 2])}, 4: {tensor([1, 2, 4])}}),\n",
       "             2: defaultdict(set, {3: {tensor([2, 3])}, 4: {tensor([2, 4])}}),\n",
       "             3: defaultdict(set, {4: {tensor([3, 4])}}),\n",
       "             4: defaultdict(set, {5: {tensor([4, 5])}})})"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as _np\n",
    "\n",
    "def shortest_paths(paths):\n",
    "    \"\"\"\n",
    "    Calculates all shortest paths between all pairs of nodes \n",
    "    based on a set of empirically observed paths.\n",
    "    \"\"\"\n",
    "    s_p = defaultdict(lambda: defaultdict(set))\n",
    "    s_p_lengths = defaultdict(lambda: defaultdict(lambda: _np.inf))\n",
    "\n",
    "    p_length = 1\n",
    "    index, edge_weights = paths.edge_index_k_weighted(k=p_length)\n",
    "    sources = index[0]\n",
    "    destinations = index[-1]\n",
    "    for e, (s, d) in enumerate(zip(sources, destinations)):\n",
    "        s = s.item()\n",
    "        d = d.item()\n",
    "        s_p_lengths[s][d] = p_length\n",
    "        s_p[s][d] = set({torch.tensor([s,d])})\n",
    "    p_length += 1\n",
    "    while True: # until max path length\n",
    "        try:\n",
    "            index, edge_weights = paths.edge_index_k_weighted(k=p_length)\n",
    "            sources = index[0, :, 0]\n",
    "            destinations = index[1, :, -1]\n",
    "            for e, (s, d) in enumerate(zip(sources, destinations)):\n",
    "                s = s.item()\n",
    "                d = d.item()\n",
    "                if p_length < s_p_lengths[s][d]:\n",
    "                    # update shortest path length\n",
    "                    s_p_lengths[s][d] = p_length\n",
    "                    # redefine set\n",
    "                    s_p[s][d] = {paths.walk_to_node_seq(index[:, e])}\n",
    "                elif p_length == s_p_lengths[s][d]:\n",
    "                    s_p[s][d].add(paths.walk_to_node_seq(index[:, e]))\n",
    "            p_length += 1\n",
    "        except IndexError:\n",
    "            print(f\"IndexError occurred. Reached maximum path length of {p_length}\")\n",
    "            break\n",
    "    return s_p\n",
    "shortest_paths(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError occurred. Reached maximum path length of 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.betweenness.<locals>.<lambda>()>,\n",
       "            {2: 3.0, 3: 1.0, 0: 0, 1: 0, 4: 0, 5: 0})"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @betweenness.register(Paths)\n",
    "def betweenness(paths, normalized=False):\n",
    "    \"\"\"Calculates the betweenness of nodes based on observed shortest paths\n",
    "    between all pairs of nodes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paths:\n",
    "        Paths object\n",
    "    normalized: bool\n",
    "        normalize such that largest value is 1.0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    \"\"\"\n",
    "    assert isinstance(paths, pp.PathData), \"argument must be an instance of pathpy.Paths\"\n",
    "    node_centralities = defaultdict(lambda: 0)\n",
    "\n",
    "    # Log.add('Calculating betweenness in paths ...', Severity.INFO)\n",
    "\n",
    "    all_paths = shortest_paths(paths)\n",
    "\n",
    "    for s in all_paths:\n",
    "        for d in all_paths[s]:\n",
    "            for p in all_paths[s][d]:\n",
    "                for x in p[1:-1]:\n",
    "                    if s != d != x:\n",
    "                        node_centralities[x.item()] += 1.0 / len(all_paths[s][d])\n",
    "    if normalized:\n",
    "        max_centr = max(node_centralities.values())\n",
    "        for v in node_centralities:\n",
    "            node_centralities[v] /= max_centr\n",
    "    # assign zero values to nodes not occurring on shortest paths\n",
    "    nodes = [v.item() for v in paths.edge_index.reshape(-1).unique(dim=0)]\n",
    "    for v in nodes:\n",
    "        node_centralities[v] += 0\n",
    "    # Log.add('finished.')\n",
    "    return node_centralities\n",
    "\n",
    "betweenness(paths,normalized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor([[0, 2, 3],\n",
       "         [2, 3, 4]]),\n",
       " 1: tensor([[0, 2],\n",
       "         [2, 3]]),\n",
       " 2: tensor([[1, 2],\n",
       "         [2, 4]]),\n",
       " 3: tensor([[4],\n",
       "         [5]])}"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths.paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 3, 1: 1, 2: 1, 3: 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths.path_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths.num_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths.node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # dist = defaultdict(lambda: defaultdict(lambda: _np.inf))\n",
    "\n",
    "    # Log.add('Calculating distance matrix based on empirical paths ...', Severity.INFO)\n",
    "    # # Node: no need to initialize shortest_path_lengths[v][v] = 0\n",
    "    # # since paths of length zero are contained in self.paths\n",
    "\n",
    "    # for v in paths.nodes:\n",
    "    #     dist[v][v] = 0\n",
    "\n",
    "    # for p_length in paths.paths:\n",
    "    #     for p in paths.paths[p_length]:\n",
    "    #         start = p[0]\n",
    "    #         end = p[-1]\n",
    "    #         if p_length < dist[start][end]:\n",
    "    #             dist[start][end] = p_length\n",
    "\n",
    "    # Log.add('finished.', Severity.INFO)\n",
    "\n",
    "    # return dist\n",
    "    ####################################################################\n",
    "# NOTE: pp2 code (see above) for 'distance_matrix' also doesn t return a matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError occurred. Reached maximum path length of 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.distance_matrix.<locals>.<lambda>()>,\n",
       "            {0: defaultdict(<function __main__.distance_matrix.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {0: 0, 2: 1, 3: 2, 4: 3}),\n",
       "             1: defaultdict(<function __main__.distance_matrix.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {1: 0, 2: 1, 4: 2}),\n",
       "             2: defaultdict(<function __main__.distance_matrix.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {2: 0, 3: 1, 4: 1}),\n",
       "             3: defaultdict(<function __main__.distance_matrix.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {3: 0, 4: 1}),\n",
       "             4: defaultdict(<function __main__.distance_matrix.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {4: 0, 5: 1}),\n",
       "             5: defaultdict(<function __main__.distance_matrix.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "                         {5: 0})})"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def distance_matrix(paths):\n",
    "    \"\"\"\n",
    "    Calculates shortest path distances between all pairs of\n",
    "    nodes based on the observed shortest paths (and subpaths)\n",
    "    \"\"\"\n",
    "    dist = defaultdict(lambda: defaultdict(lambda: _np.inf))\n",
    "    # Log.add('Calculating distance matrix based on empirical paths ...', Severity.INFO)\n",
    "    nodes = [v.item() for v in paths.edge_index.reshape(-1).unique(dim=0)] # NOTE: modify once set of nodes can be obtained from path obeject\n",
    "    for v in nodes:\n",
    "        dist[v][v] = 0\n",
    "\n",
    "    p_length = 1\n",
    "    index, edge_weights = paths.edge_index_k_weighted(k=p_length)\n",
    "    sources = index[0]\n",
    "    destinations = index[-1]\n",
    "    for e, (s, d) in enumerate(zip(sources, destinations)):\n",
    "        s = s.item()\n",
    "        d = d.item()\n",
    "        dist[s][d] = p_length\n",
    "        # s_p[s][d] = set({torch.tensor([s,d])})\n",
    "    p_length += 1\n",
    "    while True: # until max path length\n",
    "        try:\n",
    "            index, edge_weights = paths.edge_index_k_weighted(k=p_length)\n",
    "            sources = index[0, :, 0]\n",
    "            destinations = index[1, :, -1]\n",
    "            for e, (s, d) in enumerate(zip(sources, destinations)):\n",
    "                s = s.item()\n",
    "                d = d.item()\n",
    "                if p_length < dist[s][d]:\n",
    "                    # update shortest path length\n",
    "                    dist[s][d] = p_length\n",
    "            p_length += 1\n",
    "        except IndexError:\n",
    "            print(f\"IndexError occurred. Reached maximum path length of {p_length}\")\n",
    "            break\n",
    "    return dist\n",
    "distance_matrix(paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError occurred. Reached maximum path length of 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.closeness.<locals>.<lambda>()>,\n",
       "            {2: 0.7058823529411765,\n",
       "             3: 0.5294117647058824,\n",
       "             4: 1.0,\n",
       "             5: 0.35294117647058826,\n",
       "             0: 0.0,\n",
       "             1: 0.0})"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def closeness(paths, normalized=False):\n",
    "    \"\"\"Calculates the closeness of nodes based on observed shortest paths\n",
    "    between all nodes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paths: Paths\n",
    "    normalized: bool\n",
    "        normalize such that largest value is 1.0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    \"\"\"\n",
    "    node_centralities = defaultdict(lambda: 0)\n",
    "    distances = distance_matrix(paths)\n",
    "    nodes = [v.item() for v in paths.edge_index.reshape(-1).unique(dim=0)] # NOTE: modify once set of nodes can be obtained from path obeject\n",
    "\n",
    "    for x in nodes:\n",
    "        # calculate closeness centrality of x\n",
    "        for d in nodes:\n",
    "            if x != d and distances[d][x] < _np.inf:\n",
    "                node_centralities[x] += 1.0 / distances[d][x]\n",
    "\n",
    "    # assign zero values to nodes not occurring\n",
    "    \n",
    "    for v in nodes:\n",
    "        node_centralities[v] += 0\n",
    "\n",
    "    if normalized:\n",
    "        m = max(node_centralities.values())\n",
    "        for v in nodes:\n",
    "            node_centralities[v] /= m\n",
    "\n",
    "    return node_centralities\n",
    "closeness(paths, normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @distance_matrix.register(Paths)\n",
    "# def _dm(paths):\n",
    "#     \"\"\"\n",
    "#     Calculates shortest path distances between all pairs of\n",
    "#     nodes based on the observed shortest paths (and subpaths)\n",
    "#     \"\"\"\n",
    "#     dist = defaultdict(lambda: defaultdict(lambda: _np.inf))\n",
    "\n",
    "#     Log.add('Calculating distance matrix based on empirical paths ...', Severity.INFO)\n",
    "#     # Node: no need to initialize shortest_path_lengths[v][v] = 0\n",
    "#     # since paths of length zero are contained in self.paths\n",
    "\n",
    "#     for v in paths.nodes:\n",
    "#         dist[v][v] = 0\n",
    "\n",
    "#     for p_length in paths.paths:\n",
    "#         for p in paths.paths[p_length]:\n",
    "#             start = p[0]\n",
    "#             end = p[-1]\n",
    "#             if p_length < dist[start][end]:\n",
    "#                 dist[start][end] = p_length\n",
    "\n",
    "#     Log.add('finished.', Severity.INFO)\n",
    "\n",
    "#     return dist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@closeness.register(Paths)\n",
    "def _cl(paths, normalized=False):\n",
    "    \"\"\"Calculates the closeness of nodes based on observed shortest paths\n",
    "    between all nodes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paths: Paths\n",
    "    normalized: bool\n",
    "        normalize such that largest value is 1.0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    \"\"\"\n",
    "    node_centralities = defaultdict(lambda: 0)\n",
    "    distances = distance_matrix(paths)\n",
    "    nodes = paths.nodes\n",
    "\n",
    "    for x in nodes:\n",
    "        # calculate closeness centrality of x\n",
    "        for d in nodes:\n",
    "            if x != d and distances[d][x] < _np.inf:\n",
    "                node_centralities[x] += 1.0 / distances[d][x]\n",
    "\n",
    "    # assign zero values to nodes not occurring\n",
    "    \n",
    "    for v in nodes:\n",
    "        node_centralities[v] += 0\n",
    "\n",
    "    if normalized:\n",
    "        m = max(node_centralities.values())\n",
    "        for v in nodes:\n",
    "            node_centralities[v] /= m\n",
    "\n",
    "    return node_centralities\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
